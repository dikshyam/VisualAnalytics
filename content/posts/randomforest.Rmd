---
date: "2019-02-03"
title: "Random Forest Tutorial"
output: html_document
---
This tutorial explains the random forest algorithm in simple terms and elaborates on how it works by including various examples. It includes a step by step guide for running the random forest algorithm in R. In addition, it highlights the explanation of parameters used in “randomForest” R package.
Random Forest is one of the most widely used machine learning algorithms for classification. It can also be used for regression models (i.e. continuous target variable) but it mainly performs well on classification tasks (i.e. categorical target variable). It has become a lethal weapon of modern data scientists to refine the predictive model. The best part of the algorithm is that there are very few assumptions attached to it so data preparation is less challenging which makes the use of the algorithm easier and faster. It’s listed as a top algorithm (with ensembling) in Kaggle Competitions.

Let's do a quick tutorial on Random Forests:

Importing the Library Packages needed for this code
```{r Imports}
library(randomForest)
library(caret)
library(ROCR)
library(ggplot2)
library(nnet)
```

Reading the file:  
```{r importing data from csv}
data <- read.csv("election_campaign_data.csv", sep=",", header=T, strip.white = T, na.strings = c("NA","NaN","","?")) 
summary(data)
```


Dropping the following variables from the data: "cand_id", "last_name", "first_name", "twitterbirth", "facebookdate", "facebookjan", "youtubebirth".
```{r}
cols <- c("cand_id", "last_name", "first_name", "twitterbirth", "facebookdate", "facebookjan", "youtubebirth")
data[,cols] <- list(NULL)
summary(data)
```

Converting the following variables into factor variables using function as.factor(): “twitter”, “facebook”, “youtube”, “cand_ici”, and “gen_election”.
```{r}
data$twitter <- as.factor(data$twitter)
data$facebook <- as.factor(data$facebook)
data$youtube <- as.factor(data$youtube)
data$cand_ici <- as.factor(data$cand_ici)
data$gen_election <- as.factor(data$gen_election)
summary(data)
```

#### 7.	Remove all of the observations with any missing values using function complete.cases()
```{r}
data <- data[complete.cases(data),]
head(data)
```

#### 8.	Randomly assign 70% of the observations to train_data and the remaining observations to test_data (Refer to Module 6 for the code). 
```{r}
set.seed(42)
td <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[td,]
test_data <- data[setdiff(1:nrow(data), td),]
summary(train_data)
```

#### 9.	Use train_data to build a random forest classifier with 10 trees. Use library(randomForest). 
```{r}
set.seed(42)
rf <-randomForest(train_data$gen_election~., data=train_data, ntree=10, na.action=na.exclude, importance=T,
                  proximity=F) 
print(rf)
```

###### 9.1.	(2 points) What is the OOB estimate of error rate? 
OOB estimate of  error rate: 9.56%

###### 9.2.	(2 points) How many variables R tried at each split? 
No. of variables tried at each split: 5


###### 9.3.	(4 points) Now use 20 trees.
```{r}
set.seed(42)
rf <-randomForest(gen_election~., data=train_data, ntree=20, na.action=na.exclude, importance=T,
                  proximity=F) 
print(rf)
```
 
###### 9.3.1.	 What is OOB estimate of error rate? 
OOB estimate of  error rate: 7.54%


###### 9.3.2.	 How many variables R tried at each split? 
No. of variables tried at each split: 5

##### 9.4.	(4 points) Now use 30 trees. 

```{r}
set.seed(42)
rf <-randomForest(gen_election~., data=train_data, ntree=30, na.action=na.exclude, importance=T,
                  proximity=F) 
print(rf)

```

###### 9.4.1.	 What is OOB estimate of error rate? 
OOB estimate of  error rate: 7.08%

###### 9.4.2.	 How many variables R tried at each split? 
No. of variables tried at each split: 5



#### 9.5.	(2 points) Increase the number of trees in 10 increments (e.g. 40, 50, …). Using OOB error rate to evaluate your random forest classifier, how many trees would you recommend? 


```{r}
for(ntree in 10*c(1:15)) {
  set.seed(42)
  rf = randomForest(gen_election~., data=train_data, ntree=ntree, na.action=na.exclude, importance=T, proximity=F, metric='Accuracy')
  print(rf)
}
```

No. of trees = 70

```{r}
#OOB estimate of error rate at 60 trees: 6.15%
#OOB estimate of  error rate at 70 trees: 5.69%
#OOB estimate of error rate at 80 trees: 6.31%
#OOB estimate of error rate at 90 trees: 6.92%
#OOB estimate of error rate at 100 trees: 6.31%
```

######  9.6.	(2 points) Use tuneRF() function to find the best value for mtry. Here is the code: mtry <- tuneRF(train_data[-26], train_data$gen_election, ntreeTry=n,  stepFactor=1.5, improve=0.01, trace=TRUE, plot=TRUE, , na.action=na.exclude). Replace n with the number of trees you recommended in 9.5. What is the recommended value for mtry? 

```{r}
set.seed(42)
mtry <- tuneRF(train_data[-26], train_data$gen_election, ntreeTry=70,  stepFactor=1.5, improve=0.01, trace=TRUE, plot=TRUE,na.action=na.exclude)

```

Value for mtry = 4

```{r}
set.seed(42)
finalrf = randomForest(gen_election~., data=train_data, ntree=70, mtry=4, na.action=na.exclude, importance=T, proximity=F)
print(rf)
```

###### 9.7.	(2 points)  Use your recommended number of trees and mtry value to build a new random forest classifier using train_data. What is OOB estimate of error rate? 
OOB estimate of  error rate: 6.15%

###### 9.8 Use library(caret)  and the code in Module 6 to create the confusion matrix for test_data. Fill out the confusion matrix in below. Use “W” as the value of option positive in confusionMatrix() function. 
```{r}
set.seed(42)
predicted_values <- predict(rf, test_data)
confusionMatrix(predicted_values, test_data$gen_election, positive = 'W')

```

###### 9.8.1.	What is the value of accuracy? 
Accuracy : 0.92

###### 9.8.2.	What is the value of TPR? 
Sensitivity : 0.9394

###### 9.8.3.	What is the value of FPR?
1 -  0.9116 = 0.0884

##### Use the code in Module 6 to calculate AUC and create the ROC curve. 

###### 9.9.1.	 What is the value of AUC? 
AUC = 0.9845

###### 9.9.2.	Paste the ROC curve in the space below:
```{r}
set.seed(42)
predicted_values <- predict(rf, test_data,type= "prob")[,2]
pred <- prediction(predicted_values, test_data$gen_election)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]

roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="RF")
ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
  geom_ribbon(alpha=0.2) +
  geom_line(aes(y=tpr)) +
  ggtitle(paste0("ROC Curve w/ AUC=", auc))
```

##### 9.10.	(4 points) Use varImpPlot() to create the plot for variable importance. What are the type five important variables when we use MeanDecreaseAccuracy?
Opp_fund, other_pol_cmte_contrib, facebook, coh_cop, cand_pty_affiliation
```{r}
varImpPlot(rf)
```

#### 10.	Use library(nnet) and the code in Module 6 to build a neural network classifier. 

##### 10.1.	(20 points) Use 5 hidden nodes in your ANN. 
```{r}
ann <- nnet(train_data$gen_election ~ ., data=train_data, size=5, maxit=1000) 
summary(ann)
```

######  10.1.1.	How many input nodes are in the ANN? 
39
######  10.1.2.	How many weights are in the ANN? 
206 weights
######  10.1.3.	Use library(caret) and the code in Module 6 to create the confusion matrix for test_data. Fill out the confusion matrix in below. Use “W” as the value of option positive in confusionMatrix() function.

```{r}
predicted_values <- predict(ann, test_data,type= "raw")
pred <- factor( ifelse(predicted_values[,1] > 0.5, 'W', 'L') )
confusionMatrix(pred, test_data$gen_election, positive = 'W')

```

###### 10.1.4.	What is the value of sensitivity? 
Sensitivity : 0.9470
###### 10.1.5.	What is the value of specificity? 
1-0.7823 = 0.2177

###### 10.1.6.	Use the code in Module 6 to calculate AUC and create the ROC curve. 

###### 10.1.6.1.	What is the value of AUC? 
AUC= 0.9351

###### 10.1.6.2.	Paste the ROC curve in the space below:
```{r}
predicted_values <- predict(ann, test_data,type= "raw")
pred <- prediction(predicted_values, test_data$gen_election)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="ANN")
ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
  geom_ribbon(alpha=0.2) +
  geom_line(aes(y=tpr)) +
  ggtitle(paste0("ROC Curve w/ AUC=", auc))

```

##### 10.2.	(6 points) Increase the number of hidden nodes until you get the following error: “Error in nnet.default(x, y, w, entropy = TRUE, ...)	: too many (1026) weights.” Use the maximum number of hidden nodes that you can use to build your ANN classifier. 
```{r}
#ann1 <- nnet(train_data$gen_election ~ ., data=train_data, size=25, maxit=1000) 

```

ANN with 24 hidden nodes:
```{r}
ann1 <- nnet(train_data$gen_election ~ ., data=train_data, size=24, maxit=1000) 

```

###### 10.2.1.	What is the maximum number of hidden nodes that we could use? 
24
It gives an error with 25 nodes.
 
#### 10.2.2.	Use the code in Module 6 to calculate AUC and create the ROC curve. 

```{r}
predicted_values <- predict(ann1, test_data,type= "raw")
pred <- factor( ifelse(predicted_values[,1] > 0.5, 'W', 'L') )
confusionMatrix(pred, test_data$gen_election, positive = 'W')

```

###### 10.2.2.1.	What is the value of AUC? 
Auc= 0.93800

###### 10.2.2.2.	Paste the ROC curve in the space below:
```{r}
predicted_values <- predict(ann1, test_data,type= "raw")
pred <- prediction(predicted_values, test_data$gen_election)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")

auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="ANN")
ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
  geom_ribbon(alpha=0.2) +
  geom_line(aes(y=tpr)) +
  ggtitle(paste0("ROC Curve w/ AUC=", auc))

```

###### 11.	(5 points) Among the three classifiers that you built, which classifier would you finally use for predicting the election’s outcome? Please explain.

Among the three classifiers built, I'd definitely go for Random Forest with 70 trees and 4 variables over ANN with 5 layers and ANN with 24 laters because:
1. Accuracy:
Random Forest : 92.11%
ANN with 5 hidden nodes: 86.02%
ANN with 24 hidden nodes: 86.74%
RF has the maximum accuracy on the test data out of the 3 classifiers in picture.

2. Area Under Curve:
Random Forest : 98%
ANN with 5 hidden nodes: 93.5%
ANN with 24 hidden nodes: 93.5%
As the area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, the areas under ROC curves are used to compare the usefulness of tests.
Since RF has the maximum AUC, I'd use it out of the 3.

###### 12.	(10 points) The buzz from the 2008 election motivated the candidates for political offices to employ social media campaigns to get their message across. Imagine that you are an advisor to a candidate who is running for a Congressional seat. Based on your analysis, would you recommend sparing money and resources to create social media campaigns? If so, among the three social media platforms (Facebook, Twitter, and YouTube), which platform would you recommend to invest in? Please explain. You can use function ftable() in Module 3 to support your recommendation. 


Let's compute a column 'socialmedia':
```{r}
#Any kind of Social media used : 1, No Social Media: 0
socialmedia = ifelse(data['facebook'] ==1 | data['twitter']==1 | data['youtube']==1,1,0)


socialmedia=as.factor(socialmedia)
data['socialmedia']=socialmedia

comparison <- ftable(xtabs(~socialmedia+gen_election, data=data))
comparison

LostwithSM <- 100 * comparison[2] / (comparison[2] + comparison[4]) 
# %age lost after using Social Media
LostwithSM
WinwithSM <- 100 * comparison[4] / (comparison[2] + comparison[4]) 
# %age win after using Social Media
WinwithSM


```

From the results above, we see that if 93.125% of the times when social media campaigns are employed, there has been a win. We can say from the above chart that Social Media a important feature!

Moving on to choosing the platform is the most useful:

```{r}
comparison <- ftable(xtabs(~facebook+gen_election, data=data))
LwithSM <- 100 * comparison[2] / (comparison[2] + comparison[4]) 
WwithSM <- 100 * comparison[4] / (comparison[2] + comparison[4]) 
comparison
# %age lost after using Facebook
LwithSM

#%age win after using Facebook
WwithSM

#%age win after using Twitter
comparison <- ftable(xtabs(~twitter+gen_election, data=data))
LwithSM <- 100 * comparison[2] / (comparison[2] + comparison[4]) 
WwithSM <- 100 * comparison[4] / (comparison[2] + comparison[4]) 
comparison
#%age lost after using Twitter
LwithSM

#%age win after using Twitter
WwithSM


comparison <- ftable(xtabs(~youtube+gen_election, data=data))
LwithSM <- 100 * comparison[2] / (comparison[2] + comparison[4]) 
WwithSM <- 100 * comparison[4] / (comparison[2] + comparison[4]) 
comparison
#%age lost after using Youtube
LwithSM
#%age win after using Youtube
WwithSM

```

On using ftable(), for accessing which social media platform should be used for campaigns, we see that candidates using youtube win 94.9% of the time than candidates who use facebook (94.6%) and twitter (92.9%).
Let's confirm our results with RF:
Accessing social media platforms with Random Forest:
```{r}
set.seed(62)
rfSM <- randomForest(gen_election~facebook+youtube+twitter, data=data, na.action=na.exclude, importance=T, proximity=F)
varImpPlot(rfSM)
```

On using a RF to predict the target variable with the three social media platforms so as to access the importance of each platform, we find out that youtube has the highest value.
But if we check our final RF with Test Data only, we see that facebook is at the top of the 3 platforms. 
I'd say all 3 are equally important but the priority lost would be:
1. Youtube
2. Facebook
3. Twitter
(1 - highest, 3 - lowest)

###### 13.	(10 points) Given your analysis, would you agree with this statement: “Money Buys Political Power”? Please explain.
Yes, we can rightously say that "Money buys Political Power".
Let's look at our RF model varImpPlot Chart:

```{r}
varImpPlot(finalrf)
```

Both of the charts above have some kind of money related factors in the top 5 most important determining factors, either for social media or campaigning. So more the investment is by a candidate, higher are the chances of winning.


###### 14.	(10 points) Imagine that you are an advisor to a candidate who is running for a Congressional seat. Based on your analysis, what are your prescriptions for success for your candidate? Please explain.

From the variable Importance plot using MeanDecreaseAccuracy and MeanDecease Gini, the most important variables affected a candidates success are opp_fund, other_pol_cmte_contrib, coh_cop, facebook, ttl_disb, 
From this we can say that money is the deciding factor, the bigger the amount of money you put into the campaigns, the more is the probability of winning.
Another significant variable, other than money, is CAND_PTY_AFFILIATION, which one might consider looking into.
Moving on:
- We can select features based on by making a correlation matrix and taking into account the features which are highly correlated to the target variable(in the range [-1,+1])
- We can perform dimensionality reduction in case our predictor variables are highly correlated.
- We can run Logistic Regression on this data and check the coefficients to confirm our results.



