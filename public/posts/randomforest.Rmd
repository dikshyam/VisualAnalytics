---
date: "2019-02-03"
title: "Random Forest Tutorial"
output: html_document
---

This tutorial explains the random forest algorithm in simple terms and elaborates on how it works by including various examples. It includes a step by step guide for running the random forest algorithm in R. In addition, it highlights the explanation of parameters used in “randomForest” R package.
Random Forest is one of the most widely used machine learning algorithms for classification. It can also be used for regression models (i.e. continuous target variable) but it mainly performs well on classification tasks (i.e. categorical target variable). It has become a lethal weapon of modern data scientists to refine the predictive model. The best part of the algorithm is that there are very few assumptions attached to it so data preparation is less challenging which makes the use of the algorithm easier and faster. It’s listed as a top algorithm (with ensembling) in Kaggle Competitions.

Let's do a quick tutorial on Random Forests:

Importing the Library Packages needed for this code
```{r Imports}
library(randomForest)
library(caret)
library(ROCR)
library(ggplot2)
library(nnet)
```

Reading the file:  
```{r importing data from csv}
data <- read.csv("election_campaign_data.csv", sep=",", header=T, strip.white = T, na.strings = c("NA","NaN","","?")) 
summary(data)
```


Dropping the following variables from the data: "cand_id", "last_name", "first_name", "twitterbirth", "facebookdate", "facebookjan", "youtubebirth".
```{r}
cols <- c("cand_id", "last_name", "first_name", "twitterbirth", "facebookdate", "facebookjan", "youtubebirth")
data[,cols] <- list(NULL)
summary(data)
```

Converting the following variables into factor variables using function as.factor(): “twitter”, “facebook”, “youtube”, “cand_ici”, and “gen_election”.
```{r}
data$twitter <- as.factor(data$twitter)
data$facebook <- as.factor(data$facebook)
data$youtube <- as.factor(data$youtube)
data$cand_ici <- as.factor(data$cand_ici)
data$gen_election <- as.factor(data$gen_election)
summary(data)
```

Removing all of the observations with any missing values using function complete.cases()
```{r}
data <- data[complete.cases(data),]
head(data)
```

Randomly assigning 70% of the observations to train_data and the remaining observations to test_data (Refer to Module 6 for the code). 
```{r}
set.seed(42)
td <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[td,]
test_data <- data[setdiff(1:nrow(data), td),]
summary(train_data)
```

Using train_data to build a random forest classifier with 10 trees using library(randomForest). 
```{r}
set.seed(42)
rf <-randomForest(train_data$gen_election~., data=train_data, ntree=10, na.action=na.exclude, importance=T,
                  proximity=F) 
print(rf)
```

Now using 20 trees.
```{r}
set.seed(42)
rf <-randomForest(gen_election~., data=train_data, ntree=20, na.action=na.exclude, importance=T,
                  proximity=F) 
print(rf)
```
 
Now using 30 trees. 

```{r}
set.seed(42)
rf <-randomForest(gen_election~., data=train_data, ntree=30, na.action=na.exclude, importance=T,
                  proximity=F) 
print(rf)

```

We build a number of RF models by increasing the number of trees in 10 increments (e.g. 40, 50, …). Using OOB error rate to evaluate your random forest classifier, how many trees would you recommend? 

```{r}
for(ntree in 10*c(1:15)) {
  set.seed(42)
  rf = randomForest(gen_election~., data=train_data, ntree=ntree, na.action=na.exclude, importance=T, proximity=F, metric='Accuracy')
  print(rf)
}
```

Optimal No. of trees = 70

```{r}
#OOB estimate of error rate at 60 trees: 6.15%
#OOB estimate of  error rate at 70 trees: 5.69%
#OOB estimate of error rate at 80 trees: 6.31%
#OOB estimate of error rate at 90 trees: 6.92%
#OOB estimate of error rate at 100 trees: 6.31%
```

Using tuneRF() function to find the best value for mtry. What is the recommended value for mtry? 

```{r}
set.seed(42)
mtry <- tuneRF(train_data[-26], train_data$gen_election, ntreeTry=70,  stepFactor=1.5, improve=0.01, trace=TRUE, plot=TRUE,na.action=na.exclude)

```

Optimal Value for mtry = 4


Let's use your recommended number of trees and mtry value to build a new random forest classifier using train_data.

```{r}
set.seed(42)
finalrf = randomForest(gen_election~., data=train_data, ntree=70, mtry=4, na.action=na.exclude, importance=T, proximity=F)
print(rf)
```

Let's create the confusion matrix for test_data. 
```{r}
set.seed(42)
predicted_values <- predict(rf, test_data)
confusionMatrix(predicted_values, test_data$gen_election, positive = 'W')

```

Now let's calculate AUC and create the ROC curve. 

```{r}
set.seed(42)
predicted_values <- predict(rf, test_data,type= "prob")[,2]
pred <- prediction(predicted_values, test_data$gen_election)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]

roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="RF")
ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
  geom_ribbon(alpha=0.2) +
  geom_line(aes(y=tpr)) +
  ggtitle(paste0("ROC Curve w/ AUC=", auc))
```

Using varImpPlot() to create the plot for variable importance. 
```{r}
varImpPlot(rf)
```




